{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tweets = '../hierarchical_edge_bundling/tweets_marchandisation.csv'\n",
    "path_to_users = 'following/'\n",
    "\n",
    "df = pd.read_csv(path_to_tweets, sep=';')\n",
    "sorted = df.sort_values(by=\"average_hash\")\n",
    "\n",
    "users_parsed = []\n",
    "edges = \"Source;Target;LinkType;Weight;AverageHash;Sha1;Category\\n\"\n",
    "not_found = 0\n",
    "hash_weight = 1\n",
    "sha1_weight = 1\n",
    "follow_weight = 1\n",
    "\n",
    "current_ahash = sorted[\"average_hash\"][0]\n",
    "current_sha1 = sorted[\"sha1\"][0]\n",
    "current_users = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78ed1aebf97a4917a5ef98b57d90f456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1986 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Créer des liens basés sur la copublication de average hashes et le follow\n",
    "\n",
    "with tqdm(total=sorted.shape[0]) as pbar:\n",
    "    # pour chaque ligne du csv\n",
    "    for index, row in sorted.iterrows():\n",
    "        user_name = row['from_user_name']\n",
    "        ahash = row['average_hash']\n",
    "        sha1 = row['sha1']\n",
    "        category = row['category']\n",
    "\n",
    "        # si le average hash est le même que celui d'avant\n",
    "        if ahash == current_ahash:\n",
    "            # on va checker tous les users d'avant qui avaient le même average hash\n",
    "            for current_user, current_sha1 in current_users:\n",
    "                # on vérifie que ça n'est pas le même user, ni le même sha1\n",
    "                if current_user != user_name and current_sha1 != sha1:\n",
    "                    edges += f\"{user_name};{current_user};Same_hash;{hash_weight};{ahash};null;{category}\\n\"\n",
    "        # sinon on remet à 0 la liste des utilisateurs du averagehash\n",
    "        else:\n",
    "            current_users = []\n",
    "            current_ahash = ahash\n",
    "\n",
    "#        if sha1 == current_sha1:\n",
    "#            for current_user, current_sha1 in current_users:\n",
    "#                if current_user != user_name:\n",
    "#                    edges += f\"{user_name};{current_user};Same_sha1;{sha1_weight};{ahash};{sha1};{category}\\n\"\n",
    "#        else:\n",
    "#            current_users = []\n",
    "#            current_sha1 = sha1\n",
    "\n",
    "        # dans tous les cas on ajoute le user qu'on est en train de traiter avec son sha1 à la liste des users à comparer\n",
    "        current_users.append([user_name, sha1])\n",
    "\n",
    "        try:\n",
    "            # si on a pas déjà regardé les followers de l'utilisateur\n",
    "            if user_name not in users_parsed:\n",
    "                # on ouvre le fichier des followers\n",
    "                with open(path_to_users + user_name + '.json', 'r') as f:\n",
    "                    # on ne garde que les followers qui sont aussi dans le corpus\n",
    "                    data = [user['username'] for user in json.load(\n",
    "                        f) if user['username'] in list(df['from_user_name'])]\n",
    "                    # on créé un lien entre chacun de ces followers et l'utilisateur courant\n",
    "                    for user in data:\n",
    "                        edges += f\"{user_name};{user};Follow;{follow_weight};null;null;null\\n\"\n",
    "                users_parsed.append(user_name)\n",
    "        except FileNotFoundError:\n",
    "            print(user_name + \" not found!\")\n",
    "            not_found += 1\n",
    "        except json.JSONDecodeError:\n",
    "            print(user_name + \" error!\")\n",
    "            not_found += 1\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Écrire le .csv\n",
    "\n",
    "with open(f\"fol_{follow_weight}_ahash_{hash_weight}_sha1_{sha1_weight}.csv\", 'w', encoding='utf-8') as f:\n",
    "    f.write(edges)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3785a633635558c05a6b1aca16a69f0ae054170ee828414f8ee7977178feb401"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
