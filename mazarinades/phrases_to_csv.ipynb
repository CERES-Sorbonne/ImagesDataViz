{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de modules\n",
    "\n",
    "# Normaliser les chaînes de caractère\n",
    "from time import sleep\n",
    "import unicodedata as uni\n",
    "# Écrire des Regex\n",
    "import re\n",
    "# Modifier des fichiers sur le système d'exploitation\n",
    "import os\n",
    "# Lire et écrire du .csv\n",
    "import csv as csvlib\n",
    "# Comparer des chaînes de caractères\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "# Lire du .xml\n",
    "import xml.etree.ElementTree as ET\n",
    "# Définition du namespace du xml\n",
    "ns = \"http://www.tei-c.org/ns/1.0\"\n",
    "from itertools import combinations\n",
    "from tqdm.auto import tqdm\n",
    "from math import factorial\n",
    "from numpy import array, mean, percentile\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour supprimer les diacritiques\n",
    "\n",
    "def strip_accents(string):\n",
    "   return ''.join(c for c in uni.normalize('NFD', string)\n",
    "                  if uni.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " lettre de cachet dv roy envoyee a essvs pvv pnrr ſur le ſujet du plein pouuoir don iulreiei ilſeieleneur ie be ei ous pour rat e a paris, de l’imprimerie de la veufuel gvnlemot, ue des marmouets, proche l’egliſe de la magdelain agueau m\n",
      " il d ea e l e saa 3 ernergprp; peetii es el sſe ee&amp;eldou8 lettre de cachet du roy, enuoyee a meſieurs du parlement, ſrle ſujet du plein pouuoir donne par ſa majefte a monſeigneur le duc d’orleans, pour traitter auec monſieur le prince\n",
      " os amez e t feau, ureaprise eiſ qie ſn neuſi e eſlne de ce mois, noſtre tres-cher &amp; tres-ame oncle le duc d’oreans, y eſtant, nous nous ſommss reſo a ij 4 lus de luy mander, ainſi que nous luy auons deſia par deux fois ſ , quenous auions tres-agrea ble qu’il continuaſt ſon entremiſe vers noſtte couſin le prince de conde, pour le ramener en ſon deuoir, &amp; enuoyaſt vers luy pour apprendre les ſujets de ſon meſ contentement, &amp; le conuier de venir au lieu que noſtre- dit on cle auroit iuge le plus commode\n",
      " et au cas que noſtre dit omeley vouluſt bien aller, noſtre inten\n",
      " tion eſtoit qu’il fuſt ſuiuy de no ſtre couſin le mareſehal d\n",
      " l’hoſpital, &amp; des ſieurs de la mar\n",
      " guerie &amp; d’aligre de noſtre con\n",
      " ſeil, comme des ſieurs de meſme preſident de noſtre dite cour, menardeauchampray &amp; de cumont conſeillers en icelle, l’aſ ſeurant 5 ſeurant que tout ce qu’il feroit pour le bien de noſtre ſeruice nous ſeroit tres-agreable, pre nant vne entiere confiance en ſon a ffect ion: &amp; comme nous auons conſenty que noſtre-dite oncle fiſt encores ſes diligences, &amp; que nous auons iuge que nous ſerions vtilement ſeruis de ceux que nous auons ordonne de le ſuiure, nous auons bien vouluauſſi vous en faire part, ne doutant point que la compagnie n’adreſſe ſes vœux a dieu, a ce qu’il luy plaiſe donner benediction a ce qui eſt entrepris pour le bien &amp; l’aduan tage de cette couronne; sur ce nous prions cetre diuine bonte vous auoir nos amez &amp; feaux, en ſa ſaincte garde\n",
      " eſcrite a bour ges ce vnzieme octobre mil b 6 ſix cens cinquante-vn\n",
      " et plus bas, de lomenie\n",
      " nos amez &amp; ſeax les gens te nans noſtre cour de parlement a panis\n",
      "178114\n"
     ]
    }
   ],
   "source": [
    "# Extraire les phrases et créer les squelettes textuels\n",
    "\n",
    "# initier un compteur de phrases\n",
    "id_phrase = 0\n",
    "\n",
    "# créer un dictionnaire nodes[id_text + \"ph\" + id_phrase] = [id_text, id_phrase, text, label]\n",
    "nodes = {}\n",
    "# créer un dictionnaire edges[source] = [target, edge_type, lev_distance]\n",
    "edges = {}\n",
    "\n",
    "min_phr_len = 20\n",
    "max_phr_len = 2000\n",
    "\n",
    "for root_path, _, files in os.walk(\"mazarinades\"):\n",
    "    for name in files:\n",
    "        if(not name.endswith(\".xml\")):\n",
    "            continue\n",
    "        # pour chaque fichier .xml du dossier mazarinades\n",
    "        with open(os.path.join(root_path, name), 'r', encoding='utf-8') as f:\n",
    "            xml = f.read()\n",
    "            root = ET.fromstring(xml)\n",
    "            id_phrase = 0\n",
    "\n",
    "            # récupérer le titre du document pour l'afficher en label des noeuds\n",
    "            title = root.find(\".//{\" + ns + \"}title\").text\n",
    "            try:\n",
    "                label = re.sub(r'\\s+', ' ', title).replace('\\n', '')\n",
    "            except:\n",
    "                label = \"no_title\"\n",
    "\n",
    "            # récupérer l'identifiant dans id_text\n",
    "            id_text = name.replace(\".xml\", \"\")\n",
    "            # insérer le noeud titre dans le dictionnaire de noeuds\n",
    "            nodes[id_text + \"ph\" +\n",
    "                  str(id_phrase)] = [id_text, str(id_phrase), \"null\", label]\n",
    "\n",
    "            # extraire le texte et supprimer les balises, les espaces multiples, les tirets, les diacritiques et mettre tout en minuscules\n",
    "            full_text = re.sub(\n",
    "                r'\\s+', ' ', (re.sub(r'<.+?>|<\\/?|\\/?>', ' ', xml.split(\"body>\")[1])))\n",
    "            full_text = strip_accents(full_text.replace(\"¬\", \"\").lower())\n",
    "\n",
    "            # créer un noeud pour chaque phrase qui dépasse 20 caractères\n",
    "            phrases = re.findall(\"[^\\.\\?\\!]{\" + str(min_phr_len) + \",}\", full_text)\n",
    "            for phrase in phrases:\n",
    "                if id_text == \"Moreau1906_GBOOKS\":\n",
    "                    print(phrase)\n",
    "                id_phrase += 1\n",
    "                nodes[id_text + \"ph\" + str(id_phrase)] = [id_text,\n",
    "                                                     id_phrase, phrase, \"null\"]\n",
    "                # créer un lien avec la phrase précédente\n",
    "                edges[id_text + \"ph\" +\n",
    "                      str(id_phrase - 1)] = [id_text + \"ph\" + str(id_phrase), \"precedence\", \"null\"]\n",
    "\n",
    "# vérifier le nombre de phrases\n",
    "print(len(nodes))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_items' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3_/jgyqn07d54559d17pph6p1jc0000gp/T/ipykernel_9395/485512013.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# créer un extrait de 500 phrases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# définir des graduations en fonction de la taille minimale, maximale et de l'écart de levensthein minimal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_items' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "# Créer les liens de proximité entre phrases\n",
    "\n",
    "# créer une liste de textes déjà évalués\n",
    "analysed_texts = []\n",
    "min_lev_distance = 0.80\n",
    "graduations = []\n",
    "\n",
    "# définir des graduations en fonction de la taille minimale, maximale et de l'écart de levensthein minimal\n",
    "i = min_phr_len\n",
    "while i < max_phr_len:\n",
    "    graduations.append(round(i))\n",
    "    i = i/min_lev_distance\n",
    "graduations.append(max_phr_len)\n",
    "\n",
    "# répartir les noeuds dans ces groupes\n",
    "groups = [[(id, node) for id, node in nodes.items() if graduations[i] <\n",
    "           len(node[2]) < graduations[i+2]] for i in range(len(graduations)-2)]\n",
    "\n",
    "tot = sum([factorial(len(group)) // (2*factorial(len(group) - 2)) for group in groups])\n",
    "with tqdm(total = tot) as pbar:\n",
    "    for group in groups:\n",
    "        for (id1, node1), (id2, node2) in combinations(group, 2):\n",
    "            pbar.update(1)\n",
    "\n",
    "'''\n",
    "group_num = 10\n",
    "# définir la moyenne de la longueur des phrases\n",
    "avg = mean([len(node[2]) for _, node in nodes.items()])\n",
    "# récupérer la longueur de chaque phrase\n",
    "data = [len(node[2]) for _, node in nodes.items()]\n",
    "deciles = percentile(data, range(0, 101, int((1/group_num)*100)), interpolation='midpoint')\n",
    "groups = [[(id, node) for id, node in nodes.items() if deciles[i] < len(node[2]) < deciles[i+2]] for i in range(group_num-1)]\n",
    "print(len(groups[0]))\n",
    "\n",
    "with tqdm(total=factorial(len(nodes)) // (2*factorial(len(nodes) - 2))) as pbar:\n",
    "    for (id1, node1), (id2, node2) in combinations(nodes.items(), 2):\n",
    "        pbar.update(1)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# pour chaque phrase du corpus\n",
    "for id, node in nodes.items():\n",
    "    # enregistrer le texte comme analysé\n",
    "    analysed_texts.append(node[0])\n",
    "    # si la phrase analysée n'est pas un titre\n",
    "    if (node[2] != \"null\"):\n",
    "        # calculer une distance de levensthein pour toutes les phrases pas encore analysées\n",
    "        for cur_id, cur_node in nodes.items():\n",
    "            if (cur_node[0] not in analysed_texts and cur_node[2] != \"null\"):\n",
    "                lev_distance = fuzz.ratio(node[2], cur_node[2]) / 100\n",
    "        # créer des liens avec les phrases qui sont proches à 80%\n",
    "                if (lev_distance >= 0.70):\n",
    "                    print(node[2] + \" : \" + cur_node[2] +\n",
    "                          \" : \" + str(lev_distance))\n",
    "                    edges[id] = [cur_id, \"proximity\", lev_distance]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verser le contenu dans un fichier maz_phrases.csv\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3785a633635558c05a6b1aca16a69f0ae054170ee828414f8ee7977178feb401"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
