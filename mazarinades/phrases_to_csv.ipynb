{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation de modules\n",
    "\n",
    "# Normaliser les chaînes de caractère\n",
    "from time import sleep\n",
    "import unicodedata as uni\n",
    "# Écrire des Regex\n",
    "import re\n",
    "# Modifier des fichiers sur le système d'exploitation\n",
    "import os\n",
    "# Lire et écrire du .csv\n",
    "import csv as csvlib\n",
    "# Comparer des chaînes de caractères\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "# Lire du .xml\n",
    "import xml.etree.ElementTree as ET\n",
    "# Définition du namespace du xml\n",
    "ns = \"http://www.tei-c.org/ns/1.0\"\n",
    "from itertools import combinations\n",
    "from tqdm.auto import tqdm\n",
    "from math import factorial\n",
    "from numpy import array, mean, percentile\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour supprimer les diacritiques\n",
    "\n",
    "def strip_accents(string):\n",
    "   return ''.join(c for c in uni.normalize('NFD', string)\n",
    "                  if uni.category(c) != 'Mn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les phrases et créer les squelettes textuels\n",
    "\n",
    "# initier un compteur de phrases\n",
    "id_phrase = 0\n",
    "\n",
    "# créer un dictionnaire nodes[id_text + \"ph\" + id_phrase] = [id_text, id_phrase, text, label]\n",
    "nodes = {}\n",
    "# créer un dictionnaire edges[source] = [target, edge_type, lev_distance]\n",
    "edges = {}\n",
    "\n",
    "min_phr_len = 20\n",
    "max_phr_len = 2000\n",
    "\n",
    "for root_path, _, files in os.walk(\"mazarinades\"):\n",
    "    for name in files:\n",
    "        if(not name.endswith(\".xml\")):\n",
    "            continue\n",
    "        # pour chaque fichier .xml du dossier mazarinades\n",
    "        with open(os.path.join(root_path, name), 'r', encoding='utf-8') as f:\n",
    "            xml = f.read()\n",
    "            root = ET.fromstring(xml)\n",
    "            id_phrase = 0\n",
    "\n",
    "            # récupérer le titre du document pour l'afficher en label des noeuds\n",
    "            title = root.find(\".//{\" + ns + \"}title\").text\n",
    "            try:\n",
    "                label = re.sub(r'\\s+', ' ', title).replace('\\n', '')\n",
    "            except:\n",
    "                label = \"no_title\"\n",
    "\n",
    "            # récupérer l'identifiant dans id_text\n",
    "            id_text = name.replace(\".xml\", \"\")\n",
    "            # insérer le noeud titre dans le dictionnaire de noeuds\n",
    "            nodes[id_text + \"ph\" +\n",
    "                  str(id_phrase)] = [id_text, str(id_phrase), \"null\", label]\n",
    "\n",
    "            # extraire le texte et supprimer les balises, les espaces multiples, les tirets, les diacritiques et mettre tout en minuscules\n",
    "            full_text = re.sub(\n",
    "                r'\\s+', ' ', (re.sub(r'<.+?>|<\\/?|\\/?>', ' ', xml.split(\"body>\")[1])))\n",
    "            full_text = strip_accents(full_text.replace(\"¬\", \"\").lower())\n",
    "\n",
    "            # créer un noeud pour chaque phrase qui dépasse 20 caractères\n",
    "            phrases = re.findall(\"[^\\.\\?\\!]{\" + str(min_phr_len) + \",}\", full_text)\n",
    "            for phrase in phrases:\n",
    "                if id_text == \"Moreau1906_GBOOKS\":\n",
    "                    print(phrase)\n",
    "                id_phrase += 1\n",
    "                nodes[id_text + \"ph\" + str(id_phrase)] = [id_text,\n",
    "                                                     id_phrase, phrase, \"null\"]\n",
    "                # créer un lien avec la phrase précédente\n",
    "                edges[id_text + \"ph\" +\n",
    "                      str(id_phrase - 1)] = [id_text + \"ph\" + str(id_phrase), \"precedence\", \"null\"]\n",
    "\n",
    "# vérifier le nombre de phrases\n",
    "print(len(nodes))\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer les liens de proximité entre phrases\n",
    "\n",
    "# créer une liste de textes déjà évalués\n",
    "analysed_texts = []\n",
    "min_lev_distance = 0.80\n",
    "graduations = []\n",
    "\n",
    "# définir des graduations en fonction de la taille minimale, maximale et de l'écart de levensthein minimal\n",
    "i = min_phr_len\n",
    "while i < max_phr_len:\n",
    "    graduations.append(round(i))\n",
    "    i = i/min_lev_distance\n",
    "graduations.append(max_phr_len)\n",
    "\n",
    "# répartir les noeuds dans ces groupes\n",
    "groups = [[(id, node) for id, node in nodes.items() if graduations[i] <\n",
    "           len(node[2]) < graduations[i+2]] for i in range(len(graduations)-2)]\n",
    "\n",
    "tot = sum([factorial(len(group)) // (2*factorial(len(group) - 2)) for group in groups])\n",
    "with tqdm(total = tot) as pbar:\n",
    "    for group in groups:\n",
    "        for (id1, node1), (id2, node2) in combinations(group, 2):\n",
    "            pbar.update(1)\n",
    "\n",
    "'''\n",
    "group_num = 10\n",
    "# définir la moyenne de la longueur des phrases\n",
    "avg = mean([len(node[2]) for _, node in nodes.items()])\n",
    "# récupérer la longueur de chaque phrase\n",
    "data = [len(node[2]) for _, node in nodes.items()]\n",
    "deciles = percentile(data, range(0, 101, int((1/group_num)*100)), interpolation='midpoint')\n",
    "groups = [[(id, node) for id, node in nodes.items() if deciles[i] < len(node[2]) < deciles[i+2]] for i in range(group_num-1)]\n",
    "print(len(groups[0]))\n",
    "\n",
    "with tqdm(total=factorial(len(nodes)) // (2*factorial(len(nodes) - 2))) as pbar:\n",
    "    for (id1, node1), (id2, node2) in combinations(nodes.items(), 2):\n",
    "        pbar.update(1)\n",
    "'''\n",
    "\n",
    "'''\n",
    "# pour chaque phrase du corpus\n",
    "for id, node in nodes.items():\n",
    "    # enregistrer le texte comme analysé\n",
    "    analysed_texts.append(node[0])\n",
    "    # si la phrase analysée n'est pas un titre\n",
    "    if (node[2] != \"null\"):\n",
    "        # calculer une distance de levensthein pour toutes les phrases pas encore analysées\n",
    "        for cur_id, cur_node in nodes.items():\n",
    "            if (cur_node[0] not in analysed_texts and cur_node[2] != \"null\"):\n",
    "                lev_distance = fuzz.ratio(node[2], cur_node[2]) / 100\n",
    "        # créer des liens avec les phrases qui sont proches à 80%\n",
    "                if (lev_distance >= 0.70):\n",
    "                    print(node[2] + \" : \" + cur_node[2] +\n",
    "                          \" : \" + str(lev_distance))\n",
    "                    edges[id] = [cur_id, \"proximity\", lev_distance]\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verser le contenu dans un fichier maz_phrases.csv\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3785a633635558c05a6b1aca16a69f0ae054170ee828414f8ee7977178feb401"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
